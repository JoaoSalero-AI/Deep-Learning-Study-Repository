{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing an Artificial Neural Network with Mathematical Formulas Only (No Frameworks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version of Jupyter Notebook: 3.8.10\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print('Python version of Jupyter Notebook:', python_version())\n",
    "\n",
    "# To update the package, execute the code below in the terminal or comand prompt:\n",
    "# pip install -U nome_pacote\n",
    "\n",
    "# To install the exact version of a package, execute the code below on terminal or comand prompt:\n",
    "# pip install pack_name==desired_version\n",
    "\n",
    "# After it, reboot jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now only NumPy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1st Step - Coding the Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1A - Forward Propagation](https://arxiv.org/pdf/1905.07490.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Initialize Weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for random initialization of model parameters\n",
    "def initial_param(dims_input_layer):\n",
    "    \n",
    "    # Dictionary for parameters\n",
    "    parameters = {}\n",
    "    \n",
    "    # Length of layer dimensions\n",
    "    leng = len(dims_input_layer)\n",
    "    \n",
    "    # Length Loop\n",
    "    for i in range(1, leng):\n",
    "        \n",
    "        # Initialization of the weight matrix\n",
    "        parameters[\"W\" + str(i)] = np.random.randn(dims_input_layer[i], dims_input_layer[i - 1]) * 0.01\n",
    "        \n",
    "        # Bias Initialization\n",
    "        parameters[\"b\" + str(i)] = np.zeros((dims_input_layer[i], 1))\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Developing the Sigmoid Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoide Function\n",
    "def sigmoid(Z):\n",
    "    A = 1 / (1 + np.exp(-Z))\n",
    "    return A, Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Developing the ReLU Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLu Function (Rectified Linear Unit)\n",
    "def relu(Z):\n",
    "    A = abs(Z * (Z > 0))\n",
    "    return A, Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Develpping the Linear Activation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation Operation\n",
    "# A is the array with the input data\n",
    "# W is the weight matrix\n",
    "# b is the bias\n",
    "def linear_activation(A, W, b):\n",
    "    Z = np.dot(W, A) + b\n",
    "    cache = (A, W, b)\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Developing the Forward Propagation Process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward Movement \n",
    "def forward(A_prev, W, b, activation):\n",
    "    \n",
    "    # If the Sigmoid activation function is chosen\n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_activation(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "        \n",
    "    # If the Relu activation function is chosen    \n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_activation(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "        \n",
    "    cache = (linear_cache, activation_cache)\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Activation and Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Foward Propagation\n",
    "def forward_propagation(X, parameters):\n",
    "    \n",
    "    # List of previous values (cache)\n",
    "    caches = []\n",
    "    \n",
    "    # Input Data\n",
    "    A = X\n",
    "    \n",
    "    # Parameter Length\n",
    "    L = len(parameters) // 2\n",
    "   \n",
    "    # Loop\n",
    "    for i in range(1, L):\n",
    "      \n",
    "        # Saves the previous value of A\n",
    "        A_prev = A\n",
    "        \n",
    "        # Forward execution\n",
    "        A, cache = forward(A_prev, parameters[\"W\" + str(i)], parameters[\"b\" + str(i)], activation = \"relu\")\n",
    "        \n",
    "        # Saves the cache\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # Last layer output\n",
    "    A_last, cache = forward(A, parameters[\"W\" + str(L)], parameters[\"b\" + str(L)], activation = \"sigmoid\")\n",
    "    \n",
    "    # Saves cache\n",
    "    caches.append(cache)\n",
    "    \n",
    "    return(A_last, caches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Developing the Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost Function (or Error Function)\n",
    "def calculate_cost(A_last, Y):\n",
    "    \n",
    "    # Adjusts the shape of Y to obtain its length (total number of elements)\n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    # Calculate the cost comparing the real and predicted values\n",
    "    cost = (-1 / m) * np.sum((Y * np.log(A_last)) + ((1 - Y) * np.log(1 - A_last)))\n",
    "    \n",
    "    # Adjust cost shape\n",
    "    custo = np.squeeze(cost)\n",
    "    \n",
    "    return(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1B - Backward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Developing the Backward Propagation - Sigmoid Backward Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagation Sigmoid Function\n",
    "# Calculate of the derivative because we don't want the complete value of the function, but its variation\n",
    "def sigmoid_backward(da, Z):\n",
    "    \n",
    "    # Calculate the Z derivative\n",
    "    dg = (1 / (1 + np.exp(-Z))) * (1 - (1 / (1 + np.exp(-Z))))\n",
    "    \n",
    "    # Finding the change in z derivative\n",
    "    dz = da * dg\n",
    "    return dz\n",
    "\n",
    "# Compare with the sigmoid function of forward propagation\n",
    "# A = 1 / (1 + np.exp(-Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Developing the Backward Propagation - Backward Relu Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relu Function for backpropagation\n",
    "# Calculate the derivative because we don't want the complete value of the function, but its variation\n",
    "def relu_backward(da, Z):\n",
    "    \n",
    "    dg = 1 * ( Z >= 0)\n",
    "    dz = da * dg\n",
    "    return dz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Developing the Backward Propagation - Linear Activation Backward Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear activation for backpropagation \n",
    "def linear_backward_function(dz, cache):\n",
    "    \n",
    "    # Receives the cache (memory)\n",
    "    A_prev, W, b = cache\n",
    "    \n",
    "    # M Shape \n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    # Calculate the derivative of W (result of the operation with dz)\n",
    "    dW = (1 / m) * np.dot(dz, A_prev.T)\n",
    "    \n",
    "    # Calculate the derivative of b (result of the operation with dz)\n",
    "    db = (1 / m) * np.sum(dz, axis = 1, keepdims = True)\n",
    "    \n",
    "    # Calculate the operation derivative\n",
    "    dA_prev = np.dot(W.T, dz)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Developing the Backward Propagation - Ativação Linear Backward Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function which defines the activation function (Relu or Sigmoid)\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \n",
    "    # Extract cache\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    # Verifica se a ativação é relu\n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward_function(dZ, linear_cache)\n",
    "        \n",
    "    # Check if activation is Sigmoid\n",
    "    if activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward_function(dZ, linear_cache)\n",
    "        \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Activation and backpropagation - Backpropagation Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagation algorithm (calculates the gradients for the update of the weights)\n",
    "# EV = Forward's Expected Value\n",
    "# RV = Real Value\n",
    "def backward_propagation(EV, RV, caches):\n",
    "    \n",
    "    # Gradients's Dictionary\n",
    "    grads = {}\n",
    "    \n",
    "    # Datas length (in the cache)\n",
    "    L = len(caches)\n",
    "    \n",
    "    # Extracts the length for the value of m\n",
    "    m = EV.shape[1]\n",
    "    \n",
    "    # Adjust the Y shape\n",
    "    RV = RV.reshape(EV.shape)\n",
    "    \n",
    "    # Calculate the derivative of the final prediction of the network (done at the end of the forward propagation)\n",
    "    dAL = -((RV / EV) - ((1 - RV) / (1 - EV)))\n",
    "    \n",
    "    # Capture the current cache\n",
    "    current_cache = caches[L - 1]\n",
    "    \n",
    "    # Generate a list of gradients for the data, the weights and the bias (final fase of the forward propagation)\n",
    "    grads[\"dA\" + str(L - 1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n",
    "    \n",
    "    #Loop to calculate the derivative during the linear activations with the Relu\n",
    "    for l in reversed(range(L - 1)):\n",
    "        \n",
    "        # Current Cache \n",
    "        current_cache = caches[l]\n",
    "        \n",
    "        # Derivative calculation\n",
    "        dA_prev, dW, db = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, activation = \"relu\")\n",
    "        \n",
    "        #Update the gradients dictionary, using the index\n",
    "        grads[\"dA\" + str(l)] = dA_prev\n",
    "        grads[\"dW\" + str(l + 1)] = dW\n",
    "        grads[\"db\" + str(l + 1)] = db\n",
    "        \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient and Weight Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight Update Function\n",
    "def weight_update(parameters, grads, learning_rate):\n",
    "    \n",
    "    # Length of the structure with the parameters (weights and bias)\n",
    "    L = len(parameters)//2\n",
    "    \n",
    "    # Loop for the weights update\n",
    "    for l in range(L):\n",
    "        \n",
    "        # Weight update\n",
    "        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - (learning_rate * grads[\"dW\" + str(l + 1)])\n",
    "        \n",
    "        # Bias update\n",
    "        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - (learning_rate * grads[\"db\" + str(l + 1)])\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the Complete Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Neural Network Model\n",
    "def modelNN(X, Y, input_layers_dim, learning_rate = 0.0075, num_iterations = 100):\n",
    "    \n",
    "    # List to receive the cost per epoch of training\n",
    "    costs = []\n",
    "    \n",
    "    # Parameters initialization\n",
    "    parameters = initial_param(input_layers_dim)\n",
    "    \n",
    "    # Loop of iterations (epochs)\n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        # Forward Propagation\n",
    "        AL, caches = forward_propagation(X, parameters)\n",
    "        \n",
    "        # Cost Calculation\n",
    "        cost = calculate_cost(AL, Y)\n",
    "        \n",
    "        # Backward Propagation\n",
    "        gradient = backward_propagation(AL, Y, caches)\n",
    "        \n",
    "        # Weight Update\n",
    "        parameters = weight_update(parameters, gradient, learning_rate)\n",
    "        \n",
    "        # Print of the values of intermetiate cost (cost reduction indicates model learning)\n",
    "        if i % 10 == 0:\n",
    "            print(\"Cost after \" + str(i) + \" interactions is \" + str(cost))\n",
    "            costs.append(cost)\n",
    "            \n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions Function\n",
    "def predict(X, parametros):\n",
    "    AL, caches = forward_propagation(X, parametros)\n",
    "    return AL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Training the Artificial Neural Network to Predict Cancer Occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: João Salero\n",
      "\n",
      "matplotlib: 3.3.4\n",
      "ipykernel : 5.3.4\n",
      "sklearn   : 0.24.2\n",
      "sys       : 3.8.10 (default, May 19 2021, 13:12:57) [MSC v.1916 64 bit (AMD64)]\n",
      "numpy     : 1.19.5\n",
      "pandas    : 1.3.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Packages Version\n",
    "%reload_ext watermark\n",
    "%watermark -a \"João Salero\" --iversions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data\n",
    "From: https://scikit-learn.org/stable/datasets/index.html#breast-cancer-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading\n",
    "temp = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils.Bunch"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Object type\n",
    "type(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[1.799e+01, 1.038e+01, 1.228e+02, ..., 2.654e-01, 4.601e-01,\n",
       "         1.189e-01],\n",
       "        [2.057e+01, 1.777e+01, 1.329e+02, ..., 1.860e-01, 2.750e-01,\n",
       "         8.902e-02],\n",
       "        [1.969e+01, 2.125e+01, 1.300e+02, ..., 2.430e-01, 3.613e-01,\n",
       "         8.758e-02],\n",
       "        ...,\n",
       "        [1.660e+01, 2.808e+01, 1.083e+02, ..., 1.418e-01, 2.218e-01,\n",
       "         7.820e-02],\n",
       "        [2.060e+01, 2.933e+01, 1.401e+02, ..., 2.650e-01, 4.087e-01,\n",
       "         1.240e-01],\n",
       "        [7.760e+00, 2.454e+01, 4.792e+01, ..., 0.000e+00, 2.871e-01,\n",
       "         7.039e-02]]),\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
       " 'frame': None,\n",
       " 'target_names': array(['malignant', 'benign'], dtype='<U9'),\n",
       " 'DESCR': '.. _breast_cancer_dataset:\\n\\nBreast cancer wisconsin (diagnostic) dataset\\n--------------------------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 569\\n\\n    :Number of Attributes: 30 numeric, predictive attributes and the class\\n\\n    :Attribute Information:\\n        - radius (mean of distances from center to points on the perimeter)\\n        - texture (standard deviation of gray-scale values)\\n        - perimeter\\n        - area\\n        - smoothness (local variation in radius lengths)\\n        - compactness (perimeter^2 / area - 1.0)\\n        - concavity (severity of concave portions of the contour)\\n        - concave points (number of concave portions of the contour)\\n        - symmetry\\n        - fractal dimension (\"coastline approximation\" - 1)\\n\\n        The mean, standard error, and \"worst\" or largest (mean of the three\\n        worst/largest values) of these features were computed for each image,\\n        resulting in 30 features.  For instance, field 0 is Mean Radius, field\\n        10 is Radius SE, field 20 is Worst Radius.\\n\\n        - class:\\n                - WDBC-Malignant\\n                - WDBC-Benign\\n\\n    :Summary Statistics:\\n\\n    ===================================== ====== ======\\n                                           Min    Max\\n    ===================================== ====== ======\\n    radius (mean):                        6.981  28.11\\n    texture (mean):                       9.71   39.28\\n    perimeter (mean):                     43.79  188.5\\n    area (mean):                          143.5  2501.0\\n    smoothness (mean):                    0.053  0.163\\n    compactness (mean):                   0.019  0.345\\n    concavity (mean):                     0.0    0.427\\n    concave points (mean):                0.0    0.201\\n    symmetry (mean):                      0.106  0.304\\n    fractal dimension (mean):             0.05   0.097\\n    radius (standard error):              0.112  2.873\\n    texture (standard error):             0.36   4.885\\n    perimeter (standard error):           0.757  21.98\\n    area (standard error):                6.802  542.2\\n    smoothness (standard error):          0.002  0.031\\n    compactness (standard error):         0.002  0.135\\n    concavity (standard error):           0.0    0.396\\n    concave points (standard error):      0.0    0.053\\n    symmetry (standard error):            0.008  0.079\\n    fractal dimension (standard error):   0.001  0.03\\n    radius (worst):                       7.93   36.04\\n    texture (worst):                      12.02  49.54\\n    perimeter (worst):                    50.41  251.2\\n    area (worst):                         185.2  4254.0\\n    smoothness (worst):                   0.071  0.223\\n    compactness (worst):                  0.027  1.058\\n    concavity (worst):                    0.0    1.252\\n    concave points (worst):               0.0    0.291\\n    symmetry (worst):                     0.156  0.664\\n    fractal dimension (worst):            0.055  0.208\\n    ===================================== ====== ======\\n\\n    :Missing Attribute Values: None\\n\\n    :Class Distribution: 212 - Malignant, 357 - Benign\\n\\n    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\\n\\n    :Donor: Nick Street\\n\\n    :Date: November, 1995\\n\\nThis is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\\nhttps://goo.gl/U2Uwz2\\n\\nFeatures are computed from a digitized image of a fine needle\\naspirate (FNA) of a breast mass.  They describe\\ncharacteristics of the cell nuclei present in the image.\\n\\nSeparating plane described above was obtained using\\nMultisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\\nConstruction Via Linear Programming.\" Proceedings of the 4th\\nMidwest Artificial Intelligence and Cognitive Science Society,\\npp. 97-101, 1992], a classification method which uses linear\\nprogramming to construct a decision tree.  Relevant features\\nwere selected using an exhaustive search in the space of 1-4\\nfeatures and 1-3 separating planes.\\n\\nThe actual linear program used to obtain the separating plane\\nin the 3-dimensional space is that described in:\\n[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\\nProgramming Discrimination of Two Linearly Inseparable Sets\",\\nOptimization Methods and Software 1, 1992, 23-34].\\n\\nThis database is also available through the UW CS ftp server:\\n\\nftp ftp.cs.wisc.edu\\ncd math-prog/cpo-dataset/machine-learn/WDBC/\\n\\n.. topic:: References\\n\\n   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \\n     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \\n     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\\n     San Jose, CA, 1993.\\n   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \\n     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \\n     July-August 1995.\\n   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\\n     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \\n     163-171.',\n",
       " 'feature_names': array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
       "        'mean smoothness', 'mean compactness', 'mean concavity',\n",
       "        'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
       "        'radius error', 'texture error', 'perimeter error', 'area error',\n",
       "        'smoothness error', 'compactness error', 'concavity error',\n",
       "        'concave points error', 'symmetry error',\n",
       "        'fractal dimension error', 'worst radius', 'worst texture',\n",
       "        'worst perimeter', 'worst area', 'worst smoothness',\n",
       "        'worst compactness', 'worst concavity', 'worst concave points',\n",
       "        'worst symmetry', 'worst fractal dimension'], dtype='<U23'),\n",
       " 'filename': 'C:\\\\Users\\\\jimiv\\\\anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\datasets\\\\data\\\\breast_cancer.csv'}"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the object\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "dados = pd.DataFrame(columns = load_breast_cancer()[\"feature_names\"], data = load_breast_cancer()[\"data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 30)"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape\n",
    "dados.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...  worst radius  worst texture  worst perimeter  \\\n",
       "0                 0.07871  ...         25.38          17.33           184.60   \n",
       "1                 0.05667  ...         24.99          23.41           158.80   \n",
       "2                 0.05999  ...         23.57          25.53           152.50   \n",
       "3                 0.09744  ...         14.91          26.50            98.87   \n",
       "4                 0.05883  ...         22.54          16.67           152.20   \n",
       "\n",
       "   worst area  worst smoothness  worst compactness  worst concavity  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "3       567.7            0.2098             0.8663           0.6869   \n",
       "4      1575.0            0.1374             0.2050           0.4000   \n",
       "\n",
       "   worst concave points  worst symmetry  worst fractal dimension  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "3                0.2575          0.6638                  0.17300  \n",
       "4                0.1625          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data view\n",
    "dados.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mean radius                False\n",
       "mean texture               False\n",
       "mean perimeter             False\n",
       "mean area                  False\n",
       "mean smoothness            False\n",
       "mean compactness           False\n",
       "mean concavity             False\n",
       "mean concave points        False\n",
       "mean symmetry              False\n",
       "mean fractal dimension     False\n",
       "radius error               False\n",
       "texture error              False\n",
       "perimeter error            False\n",
       "area error                 False\n",
       "smoothness error           False\n",
       "compactness error          False\n",
       "concavity error            False\n",
       "concave points error       False\n",
       "symmetry error             False\n",
       "fractal dimension error    False\n",
       "worst radius               False\n",
       "worst texture              False\n",
       "worst perimeter            False\n",
       "worst area                 False\n",
       "worst smoothness           False\n",
       "worst compactness          False\n",
       "worst concavity            False\n",
       "worst concave points       False\n",
       "worst symmetry             False\n",
       "worst fractal dimension    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Missing Values Vefiring\n",
    "dados.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Variable Separation\n",
    "target = load_breast_cancer()[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "       0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Variable Target\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "357"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total records by class - Benign Cancer\n",
    "np.count_nonzero(target == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "212"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total records by class - Malignant Cancer\n",
    "np.count_nonzero(target == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels Encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels dictionary\n",
    "labels = {}\n",
    "\n",
    "# Target variable class names\n",
    "target_names = load_breast_cancer()[\"target_names\"]\n",
    "\n",
    "# Mapping\n",
    "for i in range(len(target_names)):\n",
    "    labels.update({i:target_names[i]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'malignant', 1: 'benign'}"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Labels view\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the predictor variables in X\n",
    "X = np.array(dados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.799e+01, 1.038e+01, 1.228e+02, ..., 2.654e-01, 4.601e-01,\n",
       "        1.189e-01],\n",
       "       [2.057e+01, 1.777e+01, 1.329e+02, ..., 1.860e-01, 2.750e-01,\n",
       "        8.902e-02],\n",
       "       [1.969e+01, 2.125e+01, 1.300e+02, ..., 2.430e-01, 3.613e-01,\n",
       "        8.758e-02],\n",
       "       ...,\n",
       "       [1.660e+01, 2.808e+01, 1.083e+02, ..., 1.418e-01, 2.218e-01,\n",
       "        7.820e-02],\n",
       "       [2.060e+01, 2.933e+01, 1.401e+02, ..., 2.650e-01, 4.087e-01,\n",
       "        1.240e-01],\n",
       "       [7.760e+00, 2.454e+01, 4.792e+01, ..., 0.000e+00, 2.871e-01,\n",
       "        7.039e-02]])"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View input data\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting the input and output data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, target, test_size = 0.15, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(483, 30)\n",
      "(483,)\n"
     ]
    }
   ],
   "source": [
    "# Trainig data Shape\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86, 30)\n",
      "(86,)\n"
     ]
    }
   ],
   "source": [
    "# Test data Shape\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the shape of the input data\n",
    "X_train = X_train.T\n",
    "X_test = X_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 483)\n",
      "(30, 86)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusting the output data\n",
    "y_train = y_train.reshape(1, len(y_train))\n",
    "y_test = y_test.reshape(1, len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 483)\n",
      "(1, 86)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables with the input dimensions for the number of neurons\n",
    "input_layers_dim = [X_train.shape[0], 50, 20, 5, 1]            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[30, 50, 20, 5, 1]"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_layers_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Training.\n",
      "\n",
      "Cost after 0 interactions is 0.6931276893461216\n",
      "Cost after 10 interactions is 0.692004602428229\n",
      "Cost after 20 interactions is 0.690922907585933\n",
      "Cost after 30 interactions is 0.6898806951805921\n",
      "Cost after 40 interactions is 0.6888764292353324\n",
      "Cost after 50 interactions is 0.6879086539107697\n",
      "Cost after 60 interactions is 0.6869759610356361\n",
      "Cost after 70 interactions is 0.6860769411214418\n",
      "Cost after 80 interactions is 0.6852102245827972\n",
      "Cost after 90 interactions is 0.6843748875353061\n",
      "Cost after 100 interactions is 0.6835696079577729\n",
      "Cost after 110 interactions is 0.6827931027961235\n",
      "Cost after 120 interactions is 0.682044204314441\n",
      "Cost after 130 interactions is 0.6813217926219041\n",
      "Cost after 140 interactions is 0.6806247710463252\n",
      "Cost after 150 interactions is 0.6799520828395305\n",
      "Cost after 160 interactions is 0.6793026844259258\n",
      "Cost after 170 interactions is 0.6786755387039409\n",
      "Cost after 180 interactions is 0.6780696305841306\n",
      "Cost after 190 interactions is 0.6774839554871943\n",
      "Cost after 200 interactions is 0.6769174939139998\n",
      "Cost after 210 interactions is 0.6763692183129457\n",
      "Cost after 220 interactions is 0.6758380875633951\n",
      "Cost after 230 interactions is 0.6753230379915603\n",
      "Cost after 240 interactions is 0.6748229484431806\n",
      "Cost after 250 interactions is 0.6743366482999184\n",
      "Cost after 260 interactions is 0.6738628725938233\n",
      "Cost after 270 interactions is 0.6734002501365014\n",
      "Cost after 280 interactions is 0.6729472233934642\n",
      "Cost after 290 interactions is 0.6725018901814857\n",
      "Cost after 300 interactions is 0.672061871333269\n",
      "Cost after 310 interactions is 0.6716256712892149\n",
      "Cost after 320 interactions is 0.6711902690847472\n",
      "Cost after 330 interactions is 0.6707516728668325\n",
      "Cost after 340 interactions is 0.6703050424910831\n",
      "Cost after 350 interactions is 0.6698440385654285\n",
      "Cost after 360 interactions is 0.6693606831536534\n",
      "Cost after 370 interactions is 0.6688445824622468\n",
      "Cost after 380 interactions is 0.6682806640350282\n",
      "Cost after 390 interactions is 0.667646536854372\n",
      "Cost after 400 interactions is 0.6669091825044016\n",
      "Cost after 410 interactions is 0.6660089708362475\n",
      "Cost after 420 interactions is 0.6648522699793796\n",
      "Cost after 430 interactions is 0.6633193046356294\n",
      "Cost after 440 interactions is 0.6611870657001161\n",
      "Cost after 450 interactions is 0.6580841457293248\n",
      "Cost after 460 interactions is 0.6534291945146184\n",
      "Cost after 470 interactions is 0.6464990489043116\n",
      "Cost after 480 interactions is 0.6373300511220046\n",
      "Cost after 490 interactions is 0.6284404186234395\n",
      "Cost after 500 interactions is 0.6227949082318776\n",
      "Cost after 510 interactions is 0.6194561168593734\n",
      "Cost after 520 interactions is 0.6167025796711807\n",
      "Cost after 530 interactions is 0.6140150012746101\n",
      "Cost after 540 interactions is 0.611313989709454\n",
      "Cost after 550 interactions is 0.608586097507813\n",
      "Cost after 560 interactions is 0.6058247823910345\n",
      "Cost after 570 interactions is 0.6030223139367586\n",
      "Cost after 580 interactions is 0.6001694872046309\n",
      "Cost after 590 interactions is 0.5972556410628173\n",
      "Cost after 600 interactions is 0.594266549265169\n",
      "Cost after 610 interactions is 0.5911686280916749\n",
      "Cost after 620 interactions is 0.5879744919509253\n",
      "Cost after 630 interactions is 0.584638093415038\n",
      "Cost after 640 interactions is 0.581120896250677\n",
      "Cost after 650 interactions is 0.5773713807931032\n",
      "Cost after 660 interactions is 0.5733188767070733\n",
      "Cost after 670 interactions is 0.5688648876030283\n",
      "Cost after 680 interactions is 0.5638709447462799\n",
      "Cost after 690 interactions is 0.558137416796804\n",
      "Cost after 700 interactions is 0.5513753351520603\n",
      "Cost after 710 interactions is 0.5431946719063098\n",
      "Cost after 720 interactions is 0.5331083664769684\n",
      "Cost after 730 interactions is 0.5204086922874978\n",
      "Cost after 740 interactions is 0.5041261118164617\n",
      "Cost after 750 interactions is 0.4838745724804732\n",
      "Cost after 760 interactions is 0.4620694281518469\n",
      "Cost after 770 interactions is 0.44238146489898394\n",
      "Cost after 780 interactions is 0.4991993804824889\n",
      "Cost after 790 interactions is 0.4781972716230595\n",
      "Cost after 800 interactions is 0.46790991070075083\n",
      "Cost after 810 interactions is 0.4610569178774649\n",
      "Cost after 820 interactions is 0.45772050531013697\n",
      "Cost after 830 interactions is 0.45361506733732554\n",
      "Cost after 840 interactions is 0.45099583696919304\n",
      "Cost after 850 interactions is 0.44463922889812496\n",
      "Cost after 860 interactions is 0.4399090803875356\n",
      "Cost after 870 interactions is 0.43407292191783\n",
      "Cost after 880 interactions is 0.42865965473422163\n",
      "Cost after 890 interactions is 0.4249423786104804\n",
      "Cost after 900 interactions is 0.41715133012907235\n",
      "Cost after 910 interactions is 0.4108421668579884\n",
      "Cost after 920 interactions is 0.4108841508460774\n",
      "Cost after 930 interactions is 0.4046401486123005\n",
      "Cost after 940 interactions is 0.39038186360953314\n",
      "Cost after 950 interactions is 0.38699317464296157\n",
      "Cost after 960 interactions is 0.38967395611556216\n",
      "Cost after 970 interactions is 0.386556634267126\n",
      "Cost after 980 interactions is 0.3836007472903316\n",
      "Cost after 990 interactions is 0.3804196934434457\n",
      "Cost after 1000 interactions is 0.37742472015436845\n",
      "Cost after 1010 interactions is 0.3683850151843748\n",
      "Cost after 1020 interactions is 0.36920875428684446\n",
      "Cost after 1030 interactions is 0.37101877716626275\n",
      "Cost after 1040 interactions is 0.3687319570244683\n",
      "Cost after 1050 interactions is 0.3612200243459315\n",
      "Cost after 1060 interactions is 0.35493189460426977\n",
      "Cost after 1070 interactions is 0.35466183311370786\n",
      "Cost after 1080 interactions is 0.35529649670269053\n",
      "Cost after 1090 interactions is 0.34250764309584714\n",
      "Cost after 1100 interactions is 0.3423574798707983\n",
      "Cost after 1110 interactions is 0.3400461136975574\n",
      "Cost after 1120 interactions is 0.33875574827302324\n",
      "Cost after 1130 interactions is 0.3388209014451469\n",
      "Cost after 1140 interactions is 0.3383538638186175\n",
      "Cost after 1150 interactions is 0.33603465280004824\n",
      "Cost after 1160 interactions is 0.33210108755331746\n",
      "Cost after 1170 interactions is 0.33291385704549226\n",
      "Cost after 1180 interactions is 0.33143177583325734\n",
      "Cost after 1190 interactions is 0.3281141264800462\n",
      "Cost after 1200 interactions is 0.3311380537858904\n",
      "Cost after 1210 interactions is 0.32760355579020894\n",
      "Cost after 1220 interactions is 0.32717866350341346\n",
      "Cost after 1230 interactions is 0.325565582987582\n",
      "Cost after 1240 interactions is 0.32284299028103963\n",
      "Cost after 1250 interactions is 0.31998815543869763\n",
      "Cost after 1260 interactions is 0.3168130840857848\n",
      "Cost after 1270 interactions is 0.3174602218885565\n",
      "Cost after 1280 interactions is 0.3171484021072663\n",
      "Cost after 1290 interactions is 0.3161248480122695\n",
      "Cost after 1300 interactions is 0.3126698290421665\n",
      "Cost after 1310 interactions is 0.30584689424127826\n",
      "Cost after 1320 interactions is 0.2984804715881922\n",
      "Cost after 1330 interactions is 0.29499242857082625\n",
      "Cost after 1340 interactions is 0.29428648178169603\n",
      "Cost after 1350 interactions is 0.2935024919892346\n",
      "Cost after 1360 interactions is 0.2921313642947511\n",
      "Cost after 1370 interactions is 0.2909896966405202\n",
      "Cost after 1380 interactions is 0.28999311212102635\n",
      "Cost after 1390 interactions is 0.2890799557570025\n",
      "Cost after 1400 interactions is 0.28800713880380296\n",
      "Cost after 1410 interactions is 0.2871225445981497\n",
      "Cost after 1420 interactions is 0.28626014791846344\n",
      "Cost after 1430 interactions is 0.285382652796282\n",
      "Cost after 1440 interactions is 0.2844940435669294\n",
      "Cost after 1450 interactions is 0.28360779545198994\n",
      "Cost after 1460 interactions is 0.2828525033645025\n",
      "Cost after 1470 interactions is 0.2818312779718852\n",
      "Cost after 1480 interactions is 0.2853269037490042\n",
      "Cost after 1490 interactions is 0.30493986961557107\n",
      "Cost after 1500 interactions is 0.3094539442324885\n",
      "Cost after 1510 interactions is 0.30282050051279297\n",
      "Cost after 1520 interactions is 0.2993674080299303\n",
      "Cost after 1530 interactions is 0.2929107745245341\n",
      "Cost after 1540 interactions is 0.29129509302077977\n",
      "Cost after 1550 interactions is 0.2797722930084176\n",
      "Cost after 1560 interactions is 0.278028638794657\n",
      "Cost after 1570 interactions is 0.27852240645476334\n",
      "Cost after 1580 interactions is 0.2782706418286628\n",
      "Cost after 1590 interactions is 0.27561603791903067\n",
      "Cost after 1600 interactions is 0.275264184962354\n",
      "Cost after 1610 interactions is 0.2758857331145932\n",
      "Cost after 1620 interactions is 0.27482808124203223\n",
      "Cost after 1630 interactions is 0.27456976728712224\n",
      "Cost after 1640 interactions is 0.27435510798871443\n",
      "Cost after 1650 interactions is 0.27450137323782925\n",
      "Cost after 1660 interactions is 0.2731867867126442\n",
      "Cost after 1670 interactions is 0.2727722477469726\n",
      "Cost after 1680 interactions is 0.2724542050011175\n",
      "Cost after 1690 interactions is 0.2722273706811036\n",
      "Cost after 1700 interactions is 0.2713132426912188\n",
      "Cost after 1710 interactions is 0.27100534686250155\n",
      "Cost after 1720 interactions is 0.26949948005491214\n",
      "Cost after 1730 interactions is 0.26882799848406114\n",
      "Cost after 1740 interactions is 0.2690638779373956\n",
      "Cost after 1750 interactions is 0.2683716705445526\n",
      "Cost after 1760 interactions is 0.2680839625624035\n",
      "Cost after 1770 interactions is 0.26818868151269115\n",
      "Cost after 1780 interactions is 0.26784732329017374\n",
      "Cost after 1790 interactions is 0.2669079928253476\n",
      "Cost after 1800 interactions is 0.2684276744532581\n",
      "Cost after 1810 interactions is 0.26702735003490047\n",
      "Cost after 1820 interactions is 0.2658536474510092\n",
      "Cost after 1830 interactions is 0.2659706023861455\n",
      "Cost after 1840 interactions is 0.26742844282490547\n",
      "Cost after 1850 interactions is 0.2679058028074508\n",
      "Cost after 1860 interactions is 0.2671056034699864\n",
      "Cost after 1870 interactions is 0.26671294954434394\n",
      "Cost after 1880 interactions is 0.2659630947283943\n",
      "Cost after 1890 interactions is 0.26573160970013815\n",
      "Cost after 1900 interactions is 0.26567679174378783\n",
      "Cost after 1910 interactions is 0.26338697190691956\n",
      "Cost after 1920 interactions is 0.2569475651177318\n",
      "Cost after 1930 interactions is 0.25512182343575796\n",
      "Cost after 1940 interactions is 0.2552820787059247\n",
      "Cost after 1950 interactions is 0.2545395397424658\n",
      "Cost after 1960 interactions is 0.25393962456369346\n",
      "Cost after 1970 interactions is 0.2543072641338243\n",
      "Cost after 1980 interactions is 0.25812484826794146\n",
      "Cost after 1990 interactions is 0.26132369446444753\n",
      "Cost after 2000 interactions is 0.2656662309633458\n",
      "Cost after 2010 interactions is 0.2637381371309552\n",
      "Cost after 2020 interactions is 0.26145546734949343\n",
      "Cost after 2030 interactions is 0.26002973227766596\n",
      "Cost after 2040 interactions is 0.25203276183379325\n",
      "Cost after 2050 interactions is 0.25011092339669677\n",
      "Cost after 2060 interactions is 0.25066730037774015\n",
      "Cost after 2070 interactions is 0.2500814071779783\n",
      "Cost after 2080 interactions is 0.2491774737006053\n",
      "Cost after 2090 interactions is 0.2489412691499596\n",
      "Cost after 2100 interactions is 0.2484215093655854\n",
      "Cost after 2110 interactions is 0.24738667564842923\n",
      "Cost after 2120 interactions is 0.2466527849523166\n",
      "Cost after 2130 interactions is 0.24664561053184045\n",
      "Cost after 2140 interactions is 0.2459497561015904\n",
      "Cost after 2150 interactions is 0.24724099612937647\n",
      "Cost after 2160 interactions is 0.2511848874252669\n",
      "Cost after 2170 interactions is 0.2593156549562043\n",
      "Cost after 2180 interactions is 0.2601695239339185\n",
      "Cost after 2190 interactions is 0.2560195721865799\n",
      "Cost after 2200 interactions is 0.2521592804084858\n",
      "Cost after 2210 interactions is 0.24377901868077292\n",
      "Cost after 2220 interactions is 0.24258340709118292\n",
      "Cost after 2230 interactions is 0.24253499045258625\n",
      "Cost after 2240 interactions is 0.24252707728220632\n",
      "Cost after 2250 interactions is 0.24209737098364448\n",
      "Cost after 2260 interactions is 0.24186195399068502\n",
      "Cost after 2270 interactions is 0.24161645224512218\n",
      "Cost after 2280 interactions is 0.24142744694766866\n",
      "Cost after 2290 interactions is 0.24016193881362108\n",
      "Cost after 2300 interactions is 0.2400902696263512\n",
      "Cost after 2310 interactions is 0.24042395748680173\n",
      "Cost after 2320 interactions is 0.2395802734133817\n",
      "Cost after 2330 interactions is 0.2394384639306854\n",
      "Cost after 2340 interactions is 0.23993141045493285\n",
      "Cost after 2350 interactions is 0.23976020057201236\n",
      "Cost after 2360 interactions is 0.24311572024039865\n",
      "Cost after 2370 interactions is 0.2570761503745387\n",
      "Cost after 2380 interactions is 0.2540600960882057\n",
      "Cost after 2390 interactions is 0.24693092462173064\n",
      "Cost after 2400 interactions is 0.23881231386843163\n",
      "Cost after 2410 interactions is 0.23779114170265073\n",
      "Cost after 2420 interactions is 0.2377904200871366\n",
      "Cost after 2430 interactions is 0.2370636130911316\n",
      "Cost after 2440 interactions is 0.23725468719309825\n",
      "Cost after 2450 interactions is 0.23659538133777702\n",
      "Cost after 2460 interactions is 0.23639988579377139\n",
      "Cost after 2470 interactions is 0.23826252941673282\n",
      "Cost after 2480 interactions is 0.23849839207821888\n",
      "Cost after 2490 interactions is 0.23629994531534604\n",
      "Cost after 2500 interactions is 0.23546461810310285\n",
      "Cost after 2510 interactions is 0.23498322585389628\n",
      "Cost after 2520 interactions is 0.23450974536318162\n",
      "Cost after 2530 interactions is 0.2347823292729276\n",
      "Cost after 2540 interactions is 0.23711837742659764\n",
      "Cost after 2550 interactions is 0.24244686600687837\n",
      "Cost after 2560 interactions is 0.2471315603905714\n",
      "Cost after 2570 interactions is 0.23838891445232416\n",
      "Cost after 2580 interactions is 0.23489590744821173\n",
      "Cost after 2590 interactions is 0.2340432402903439\n",
      "Cost after 2600 interactions is 0.23396625752880038\n",
      "Cost after 2610 interactions is 0.23438773000588647\n",
      "Cost after 2620 interactions is 0.23370197469271417\n",
      "Cost after 2630 interactions is 0.23472503394221572\n",
      "Cost after 2640 interactions is 0.23341805170284774\n",
      "Cost after 2650 interactions is 0.234043861882242\n",
      "Cost after 2660 interactions is 0.23356100978015398\n",
      "Cost after 2670 interactions is 0.23296796216788074\n",
      "Cost after 2680 interactions is 0.2318770534391503\n",
      "Cost after 2690 interactions is 0.23201809548848457\n",
      "Cost after 2700 interactions is 0.23308835170357323\n",
      "Cost after 2710 interactions is 0.23195912921843592\n",
      "Cost after 2720 interactions is 0.23294263539982646\n",
      "Cost after 2730 interactions is 0.23243735400594856\n",
      "Cost after 2740 interactions is 0.23191694421828604\n",
      "Cost after 2750 interactions is 0.23151583145477259\n",
      "Cost after 2760 interactions is 0.23124172005763838\n",
      "Cost after 2770 interactions is 0.2306428072054355\n",
      "Cost after 2780 interactions is 0.23020805248118056\n",
      "Cost after 2790 interactions is 0.23068833172173933\n",
      "Cost after 2800 interactions is 0.2293039190144421\n",
      "Cost after 2810 interactions is 0.2295641206740946\n",
      "Cost after 2820 interactions is 0.22941781271939551\n",
      "Cost after 2830 interactions is 0.22952744943241937\n",
      "Cost after 2840 interactions is 0.2291825504957307\n",
      "Cost after 2850 interactions is 0.2284911225326165\n",
      "Cost after 2860 interactions is 0.22823953692870427\n",
      "Cost after 2870 interactions is 0.22852448808312564\n",
      "Cost after 2880 interactions is 0.2273273314201794\n",
      "Cost after 2890 interactions is 0.22804409897136804\n",
      "Cost after 2900 interactions is 0.2274497526786875\n",
      "Cost after 2910 interactions is 0.2275820554865275\n",
      "Cost after 2920 interactions is 0.2275846254654288\n",
      "Cost after 2930 interactions is 0.2273360178400583\n",
      "Cost after 2940 interactions is 0.22698088720907994\n",
      "Cost after 2950 interactions is 0.2277158524583237\n",
      "Cost after 2960 interactions is 0.22717975073758306\n",
      "Cost after 2970 interactions is 0.22634091993084487\n",
      "Cost after 2980 interactions is 0.22450226267779566\n",
      "Cost after 2990 interactions is 0.22058939764258867\n",
      "\n",
      "Training Completed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model Training\n",
    "\n",
    "print(\"\\nStarting Training.\\n\")\n",
    "\n",
    "parameters, cost = modelNN(X = X_train, \n",
    "                             Y = y_train, \n",
    "                             input_layers_dim = input_layers_dim, \n",
    "                             num_iterations = 3000, \n",
    "                             learning_rate = 0.0075)\n",
    "\n",
    "print(\"\\nTraining Completed.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1dc15ec25e0>]"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Error plot during training\n",
    "plt.plot(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions with training data\n",
    "y_pred_train = predict(X_train, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.85406289e-01, 8.81108619e-01, 1.09085261e-08, 2.45285687e-02,\n",
       "        8.85408502e-01, 7.54637247e-12, 8.85405295e-01, 8.85409514e-01,\n",
       "        8.85408511e-01, 2.65011422e-04, 8.85407396e-01, 8.85408076e-01,\n",
       "        8.77427009e-01, 8.85409032e-01, 6.68853024e-01, 8.85408216e-01,\n",
       "        2.37167795e-02, 2.73546142e-03, 6.51933601e-01, 1.97043414e-02,\n",
       "        8.82635260e-01, 8.85407642e-01, 8.75995605e-01, 8.68990745e-01,\n",
       "        7.94789075e-01, 9.85120594e-12, 8.85407572e-01, 2.01663859e-05,\n",
       "        8.81446458e-01, 8.31380482e-01, 8.85408371e-01, 8.85409148e-01,\n",
       "        1.84058354e-02, 1.05943283e-08, 3.90920079e-02, 1.31054104e-01,\n",
       "        8.77429716e-01, 4.44770308e-03, 8.85407749e-01, 4.87264321e-01,\n",
       "        8.85407551e-01, 3.32905417e-01, 8.85408315e-01, 7.86796197e-01,\n",
       "        8.85408779e-01, 1.40603191e-03, 8.85409651e-01, 8.85407689e-01,\n",
       "        7.10362167e-01, 8.85407661e-01, 8.12865094e-01, 5.73224461e-02,\n",
       "        8.77553232e-01, 1.96689813e-05, 7.51398542e-01, 4.69049973e-04,\n",
       "        8.85407762e-01, 1.18820480e-02, 8.85408209e-01, 5.86726288e-05,\n",
       "        7.88125417e-01, 4.20452588e-01, 1.69585707e-03, 8.85407519e-01,\n",
       "        1.01287978e-04, 6.81697973e-01, 8.32757543e-01, 5.78175805e-01,\n",
       "        8.85409851e-01, 6.00541056e-02, 3.65314357e-05, 8.12336889e-01,\n",
       "        2.24841325e-01, 8.85409947e-01, 8.49031474e-01, 1.40750710e-01,\n",
       "        8.85407590e-01, 8.61303958e-01, 2.56771321e-08, 4.74544676e-05,\n",
       "        8.85407859e-01, 7.27808067e-02, 8.85406045e-01, 8.38008224e-01,\n",
       "        8.56109298e-02, 1.05382382e-04, 7.89009935e-01, 8.85409094e-01,\n",
       "        8.85406800e-01, 8.81666285e-01, 8.85407128e-01, 2.80281105e-07,\n",
       "        2.99871887e-03, 8.85408658e-01, 8.85409712e-01, 8.85408582e-01,\n",
       "        7.12351665e-01, 8.85406893e-01, 8.09415988e-01, 4.72955174e-01,\n",
       "        8.85408574e-01, 3.28708642e-03, 1.95787028e-05, 5.37639600e-05,\n",
       "        1.58911716e-05, 8.37054329e-07, 9.67760954e-09, 8.17870122e-01,\n",
       "        8.85407369e-01, 8.51725594e-01, 2.36464258e-01, 1.74824210e-02,\n",
       "        1.27330187e-02, 7.36110311e-01, 7.41020144e-01, 6.04634622e-01,\n",
       "        3.25275862e-03, 7.82437789e-01, 8.37150322e-01, 8.73429764e-01,\n",
       "        1.13363192e-02, 6.44165067e-01, 8.36843902e-01, 8.85409739e-01,\n",
       "        8.85409450e-01, 7.66681831e-01, 4.54564831e-03, 8.85407387e-01,\n",
       "        8.85408233e-01, 8.85406982e-01, 8.85407804e-01, 6.99269437e-01,\n",
       "        8.85409183e-01, 2.83176952e-11, 8.85408777e-01, 3.27227188e-07,\n",
       "        8.85409799e-01, 8.85409173e-01, 8.27630585e-01, 1.57733977e-02,\n",
       "        8.85407826e-01, 7.95266141e-01, 8.18145108e-01, 1.50403423e-12,\n",
       "        1.44155941e-03, 4.17445244e-01, 8.81737990e-01, 8.85409546e-01,\n",
       "        8.85407626e-01, 8.85408320e-01, 6.92155056e-02, 8.85408412e-01,\n",
       "        1.53163985e-05, 8.58901859e-01, 8.85408394e-01, 1.28431649e-02,\n",
       "        7.58740753e-03, 4.13037406e-03, 3.49117835e-01, 8.85408247e-01,\n",
       "        3.76504083e-02, 2.62532751e-01, 7.40924455e-01, 8.85408022e-01,\n",
       "        8.61087017e-01, 6.15699395e-04, 8.85408748e-01, 4.40857878e-07,\n",
       "        8.85409939e-01, 7.83864178e-01, 8.85408989e-01, 8.85409963e-01,\n",
       "        8.53398293e-01, 2.61460464e-01, 6.32009547e-01, 8.85408877e-01,\n",
       "        8.85405702e-01, 8.27206858e-01, 2.84525636e-04, 8.44101820e-01,\n",
       "        8.85409371e-01, 8.85408298e-01, 1.39155163e-01, 8.79804291e-01,\n",
       "        8.34217946e-01, 2.56533953e-01, 8.85407669e-01, 7.50804121e-01,\n",
       "        8.85409529e-01, 7.86719582e-06, 8.85408715e-01, 8.40716323e-01,\n",
       "        8.30705080e-01, 8.26749161e-01, 2.61950212e-01, 8.53557241e-01,\n",
       "        1.02362337e-07, 1.69613858e-03, 8.85407773e-01, 3.80731526e-10,\n",
       "        1.33202816e-07, 8.85407847e-01, 3.18619074e-02, 1.31721100e-03,\n",
       "        5.77722182e-04, 6.74161899e-05, 8.85408251e-01, 8.09260461e-01,\n",
       "        3.20150331e-06, 8.85409140e-01, 1.03654405e-25, 8.85408139e-01,\n",
       "        1.88596871e-01, 8.85408459e-01, 8.62366358e-01, 5.06792393e-01,\n",
       "        8.78607263e-01, 8.85409666e-01, 6.08295196e-01, 6.30336033e-01,\n",
       "        8.85411082e-01, 7.59983881e-01, 8.41026637e-01, 6.86930449e-01,\n",
       "        1.16116617e-02, 3.06722736e-01, 1.20842106e-05, 8.11426735e-01,\n",
       "        3.54973909e-01, 6.44260192e-10, 2.50678910e-04, 7.15117845e-01,\n",
       "        8.85407141e-01, 8.39337921e-01, 8.65691136e-01, 8.82418547e-01,\n",
       "        8.85346648e-01, 8.85405170e-01, 8.85409203e-01, 8.85408934e-01,\n",
       "        6.94305532e-01, 8.85407365e-01, 8.85407235e-01, 5.98763469e-01,\n",
       "        8.19412441e-01, 8.81111429e-01, 7.35896549e-01, 8.85408944e-01,\n",
       "        7.49001559e-01, 8.28869108e-01, 2.08172470e-01, 8.85408361e-01,\n",
       "        8.10345520e-01, 8.85408693e-01, 8.83540734e-01, 8.85408583e-01,\n",
       "        1.49471723e-01, 2.65484760e-02, 6.46079713e-01, 8.85407928e-01,\n",
       "        3.69487499e-05, 3.61448310e-04, 8.49128499e-01, 1.16295830e-13,\n",
       "        8.85407983e-01, 1.06902272e-01, 1.04431505e-01, 5.79961086e-02,\n",
       "        5.95408687e-01, 8.85409659e-01, 8.85407697e-01, 9.47051885e-02,\n",
       "        4.45505496e-05, 8.83156178e-01, 8.85404703e-01, 5.17891495e-05,\n",
       "        9.02527429e-07, 8.85402181e-01, 5.52496568e-01, 8.72834418e-01,\n",
       "        7.15383335e-01, 2.83000430e-05, 3.89185304e-03, 8.85408095e-01,\n",
       "        8.41253366e-01, 8.85410516e-01, 8.85407985e-01, 7.36833137e-01,\n",
       "        6.51066544e-01, 8.85406604e-01, 8.85407490e-01, 8.85407877e-01,\n",
       "        8.51481912e-01, 1.52966521e-15, 7.52368314e-02, 8.85408339e-01,\n",
       "        8.06850605e-01, 1.97392740e-04, 6.08803519e-01, 8.85406937e-01,\n",
       "        8.73175043e-01, 8.23060748e-01, 8.85408670e-01, 8.85408298e-01,\n",
       "        1.55471610e-02, 8.85409497e-01, 8.27083696e-01, 8.84622168e-01,\n",
       "        7.84568459e-01, 4.54308930e-04, 8.73210945e-02, 8.85406412e-01,\n",
       "        8.85407939e-01, 8.85408474e-01, 7.85961566e-01, 6.97896011e-04,\n",
       "        6.57200399e-01, 8.83368030e-01, 8.61768445e-01, 5.33088281e-01,\n",
       "        2.14558420e-04, 8.85408131e-01, 7.23895989e-05, 1.21734861e-07,\n",
       "        7.97367729e-01, 8.51922766e-01, 8.85409965e-01, 8.85408273e-01,\n",
       "        8.23085087e-01, 6.44165292e-01, 7.56649324e-07, 8.85407448e-01,\n",
       "        4.54420542e-03, 8.48688976e-01, 8.85409513e-01, 8.53792954e-01,\n",
       "        8.85407210e-01, 8.85408645e-01, 9.03141313e-03, 8.85408839e-01,\n",
       "        7.79480079e-01, 7.23884862e-01, 4.91777453e-03, 8.85408811e-01,\n",
       "        7.54778645e-01, 2.33885609e-01, 5.23208540e-01, 8.85408276e-01,\n",
       "        8.85407396e-01, 7.02943336e-01, 5.80965545e-03, 8.85407357e-01,\n",
       "        8.85406375e-01, 8.40406129e-01, 5.23946341e-05, 8.85408118e-01,\n",
       "        6.65709843e-02, 8.85409761e-01, 1.69112228e-04, 8.85407609e-01,\n",
       "        2.09408296e-01, 8.85407012e-01, 3.18827579e-01, 8.85408124e-01,\n",
       "        1.55697117e-04, 7.96408608e-04, 8.85408044e-01, 6.08210479e-01,\n",
       "        6.21598212e-05, 8.85408083e-01, 8.22729368e-01, 1.09040299e-01,\n",
       "        8.85408577e-01, 8.42383627e-01, 8.50926556e-01, 8.85407370e-01,\n",
       "        7.95202036e-01, 4.81648431e-01, 8.85408483e-01, 8.85408628e-01,\n",
       "        6.65671966e-01, 8.36244419e-01, 6.89013189e-01, 3.57985659e-04,\n",
       "        8.85408087e-01, 8.85407230e-01, 8.85407846e-01, 8.13381920e-15,\n",
       "        8.85409967e-01, 4.41112607e-01, 8.85408302e-01, 8.81375809e-01,\n",
       "        6.16509142e-01, 8.31849750e-21, 6.88084234e-14, 8.85408518e-01,\n",
       "        3.43187370e-04, 8.52529423e-01, 2.44454189e-08, 8.85409172e-01,\n",
       "        8.85409402e-01, 3.96474324e-01, 3.25041468e-15, 1.99035069e-05,\n",
       "        8.85407224e-01, 3.14949211e-01, 8.41677010e-01, 3.85912281e-02,\n",
       "        8.85060660e-01, 6.42885971e-04, 7.64489719e-01, 5.56228024e-08,\n",
       "        8.80256805e-01, 8.85407331e-01, 1.58051444e-04, 8.85408988e-01,\n",
       "        8.85407125e-01, 8.20375372e-01, 8.85408918e-01, 8.84074212e-01,\n",
       "        8.63134702e-01, 8.85408618e-01, 8.85408656e-01, 6.18054262e-06,\n",
       "        8.85409107e-01, 2.12232572e-05, 8.85408513e-01, 1.01814084e-05,\n",
       "        8.85407539e-01, 4.17572007e-08, 8.85408402e-01, 4.75019047e-02,\n",
       "        8.85409609e-01, 9.96230058e-15, 3.18154016e-04, 6.65871770e-04,\n",
       "        8.44546311e-01, 6.50149902e-01, 8.85408141e-01, 7.97193589e-01,\n",
       "        1.25314499e-05, 8.24999949e-01, 3.80505041e-09, 8.78416563e-01,\n",
       "        6.40831807e-10, 2.12742397e-03, 8.42188547e-01, 3.87365035e-09,\n",
       "        8.85409317e-01, 8.45278722e-01, 7.80749461e-01, 8.78811408e-05,\n",
       "        8.85408775e-01, 8.85407494e-01, 8.85408634e-01, 8.67965191e-01,\n",
       "        8.26574524e-01, 7.97518778e-01, 8.85408370e-01, 8.44078536e-01,\n",
       "        8.85407834e-01, 2.79130358e-03, 1.42336067e-05, 8.85405619e-01,\n",
       "        7.18715729e-01, 5.77274975e-05, 1.27826036e-02, 3.97166696e-02,\n",
       "        4.04356614e-01, 2.68382966e-01, 8.85407279e-01, 8.85409727e-01,\n",
       "        8.85409459e-01, 8.69045830e-01, 4.69278113e-01, 8.85408759e-01,\n",
       "        1.91984122e-01, 2.77230565e-06, 7.74178054e-01, 2.13621107e-01,\n",
       "        2.19781956e-01, 3.92457398e-08, 3.14174916e-08]])"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predictions view\n",
    "y_pred_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusting the shape in training\n",
    "y_pred_train = y_pred_train.reshape(-1)\n",
    "y_train = y_train.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True, False, False,  True, False,  True,  True,  True,\n",
       "       False,  True,  True,  True,  True,  True,  True, False, False,\n",
       "        True, False,  True,  True,  True,  True,  True, False,  True,\n",
       "       False,  True,  True,  True,  True, False, False, False, False,\n",
       "        True, False,  True, False,  True, False,  True,  True,  True,\n",
       "       False,  True,  True,  True,  True,  True, False,  True, False,\n",
       "        True, False,  True, False,  True, False,  True, False, False,\n",
       "        True, False,  True,  True,  True,  True, False, False,  True,\n",
       "       False,  True,  True, False,  True,  True, False, False,  True,\n",
       "       False,  True,  True, False, False,  True,  True,  True,  True,\n",
       "        True, False, False,  True,  True,  True,  True,  True,  True,\n",
       "       False,  True, False, False, False, False, False, False,  True,\n",
       "        True,  True, False, False, False,  True,  True,  True, False,\n",
       "        True,  True,  True, False,  True,  True,  True,  True,  True,\n",
       "       False,  True,  True,  True,  True,  True,  True, False,  True,\n",
       "       False,  True,  True,  True, False,  True,  True,  True, False,\n",
       "       False, False,  True,  True,  True,  True, False,  True, False,\n",
       "        True,  True, False, False, False, False,  True, False, False,\n",
       "        True,  True,  True, False,  True, False,  True,  True,  True,\n",
       "        True,  True, False,  True,  True,  True,  True, False,  True,\n",
       "        True,  True, False,  True,  True, False,  True,  True,  True,\n",
       "       False,  True,  True,  True,  True, False,  True, False, False,\n",
       "        True, False, False,  True, False, False, False, False,  True,\n",
       "        True, False,  True, False,  True, False,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "       False, False,  True, False, False, False,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True, False,  True,\n",
       "        True,  True,  True,  True, False, False,  True,  True, False,\n",
       "       False,  True, False,  True, False, False, False,  True,  True,\n",
       "        True, False, False,  True,  True, False, False,  True,  True,\n",
       "        True,  True, False, False,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True, False, False,  True,  True,\n",
       "       False,  True,  True,  True,  True,  True,  True, False,  True,\n",
       "        True,  True,  True, False, False,  True,  True,  True,  True,\n",
       "       False,  True,  True,  True,  True, False,  True, False, False,\n",
       "        True,  True,  True,  True,  True,  True, False,  True, False,\n",
       "        True,  True,  True,  True,  True, False,  True,  True,  True,\n",
       "       False,  True,  True, False,  True,  True,  True,  True, False,\n",
       "        True,  True,  True, False,  True, False,  True, False,  True,\n",
       "       False,  True, False,  True, False, False,  True,  True, False,\n",
       "        True,  True, False,  True,  True,  True,  True,  True, False,\n",
       "        True,  True,  True,  True,  True, False,  True,  True,  True,\n",
       "       False,  True, False,  True,  True,  True, False, False,  True,\n",
       "       False,  True, False,  True,  True, False, False, False,  True,\n",
       "       False,  True, False,  True, False,  True, False,  True,  True,\n",
       "       False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "       False,  True, False,  True, False,  True, False,  True, False,\n",
       "        True, False, False, False,  True,  True,  True,  True, False,\n",
       "        True, False,  True, False, False,  True, False,  True,  True,\n",
       "        True, False,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True, False, False,  True,  True, False, False, False,\n",
       "       False, False,  True,  True,  True,  True, False,  True, False,\n",
       "       False,  True, False, False, False, False])"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_train > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert predictions to class binary value (0 or 1, using the probability value of 0.5 as threshold)\n",
    "y_pred_train = 1 * (y_pred_train > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "       1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,\n",
       "       1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,\n",
       "       1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,\n",
       "       1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,\n",
       "       0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,\n",
       "       0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating accuracy by comparing actual value with predicted value\n",
    "acc_train = sum(1 * (y_pred_train == y_train)) / len(y_pred_train) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in training data: 92.54658385093167\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy in training data: \" + str(acc_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Malignant       0.92      0.87      0.90       182\n",
      "      Benign       0.93      0.96      0.94       301\n",
      "\n",
      "    accuracy                           0.93       483\n",
      "   macro avg       0.93      0.92      0.92       483\n",
      "weighted avg       0.93      0.93      0.93       483\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train, y_pred_train, target_names = ['Malignant', 'Benign']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions with the model using test data\n",
    "y_pred_test = predict(X_test, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.84057030e-02, 6.70504474e-01, 7.60501115e-02, 4.28699216e-02,\n",
       "        8.85409531e-01, 7.34481976e-01, 8.58729658e-01, 2.79864886e-06,\n",
       "        6.27284530e-02, 8.85408591e-01, 8.85407654e-01, 8.85407926e-01,\n",
       "        8.85408649e-01, 8.85407365e-01, 8.13763706e-01, 8.85408566e-01,\n",
       "        8.85405492e-01, 8.80186186e-01, 7.84320493e-11, 1.45916079e-04,\n",
       "        7.20961568e-01, 8.85407970e-01, 8.77104780e-01, 8.85407954e-01,\n",
       "        8.60030972e-01, 5.34941042e-01, 8.69049926e-01, 8.85407632e-01,\n",
       "        1.81405687e-02, 5.02636615e-12, 8.64213345e-01, 1.72789020e-15,\n",
       "        1.73072442e-02, 8.65555671e-01, 8.85408730e-01, 8.85407994e-01,\n",
       "        8.00579507e-01, 8.85406684e-01, 7.57471812e-01, 4.82056585e-01,\n",
       "        7.43360549e-09, 2.79322291e-01, 4.72168667e-06, 4.13312892e-02,\n",
       "        6.67250657e-01, 8.85407370e-01, 2.10883044e-04, 2.27743055e-07,\n",
       "        3.58906310e-03, 1.07695750e-03, 7.25082336e-01, 8.85409706e-01,\n",
       "        8.85407397e-01, 8.85405279e-01, 7.50336552e-01, 8.24319789e-01,\n",
       "        8.02290696e-01, 8.85408169e-01, 8.85408569e-01, 4.55895346e-16,\n",
       "        1.03214580e-25, 7.48115265e-01, 1.03925908e-04, 3.26442035e-01,\n",
       "        8.53231197e-01, 8.49357681e-01, 8.85409063e-01, 8.85408050e-01,\n",
       "        8.85408410e-01, 5.72678038e-01, 8.65038638e-06, 8.84693198e-01,\n",
       "        7.52699600e-01, 8.85408658e-01, 8.85407936e-01, 8.85409340e-01,\n",
       "        6.60429711e-03, 8.85407191e-01, 8.70621545e-01, 8.74336596e-01,\n",
       "        8.85409462e-01, 2.59549183e-01, 3.58263291e-06, 7.40572577e-01,\n",
       "        8.85408976e-01, 8.85408749e-01]])"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View Data\n",
    "y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shapes settings\n",
    "y_pred_test = y_pred_test.reshape(-1)\n",
    "y_test = y_test.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting predictions to class binary value (0 or 1, using the probability value of 0.5 as threshold)\n",
    "y_pred_test = 1 * (y_pred_test > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "       1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View predictions    \n",
    "y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the accuracy\n",
    "accuracy = sum(1 * (y_pred_test == y_test)) / len(y_pred_test) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data accuracy: 93.02325581395348\n"
     ]
    }
   ],
   "source": [
    "print(\"Test data accuracy: \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Malignant       0.93      0.87      0.90        30\n",
      "      Benign       0.93      0.96      0.95        56\n",
      "\n",
      "    accuracy                           0.93        86\n",
      "   macro avg       0.93      0.92      0.92        86\n",
      "weighted avg       0.93      0.93      0.93        86\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_test, target_names = ['Malignant', 'Benign']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

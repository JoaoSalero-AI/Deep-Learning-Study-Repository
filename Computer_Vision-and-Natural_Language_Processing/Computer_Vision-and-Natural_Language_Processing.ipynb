{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Computer Vision and Natural Language Processing for Automatic Subtitle Generation From an Image</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing and Loading Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Version\n",
    "from platform import python_version\n",
    "print('Python Verison in this Jupyter Notebook:', python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To update a package, execute the following command in the terminal or command prompt:\n",
    "# pip install -U pack_name\n",
    "\n",
    "# To install the exact version of a package, execute the following command in the terminal or command prompt:\n",
    "# pip install pack_name==desired_version\n",
    "\n",
    "# After installing or updating the package, restart the jupyter notebook.\n",
    "\n",
    "# Watermark package \n",
    "# !pip install -q -U "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import re\n",
    "import cv2\n",
    "import pickle\n",
    "import matplotlib\n",
    "import tensorflow\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.applications.xception import Xception, preprocess_input, decode_predictions\n",
    "from tensorflow.keras.layers import Add\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Embedding, LSTM\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages versions in this Jupyter Notebook \n",
    "%reload_ext watermark\n",
    "%watermark -a \"Joao Salero\" --iversions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Understanding the Subtitle Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to open and read a file\n",
    "def read_file(path):\n",
    "    with open(path) as file:\n",
    "        data = file.read()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading subtitles from Flickr8k.token.txt file\n",
    "data_captions = read_file(\"/media/datasets/ComputerVision/Cap10/dados/texto/tokens.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract each line from the file\n",
    "caption = data_captions.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the last blanc line\n",
    "caption = caption[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print an example subtitle\n",
    "print(caption[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total of subtittles = \" + str(len(caption)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing Legends in Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty Dictionary\n",
    "dict_content = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through subtitles\n",
    "for line in caption:\n",
    "    \n",
    "    # Split\n",
    "    ID, caption = line.split('\\t')\n",
    "    imageID = ID.split('.')[0]\n",
    "\n",
    "    # Include image if it does not in the Dictionary\n",
    "    if dict_content.get(imageID) is None:\n",
    "        dict_content[imageID] = []\n",
    "\n",
    "    # Append\n",
    "    dict_content[imageID].append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the subtitles of the index image 1\n",
    "print(dict_content[caption[1].split('.')[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another example: Print image captions from ID=1002674143_1b742ab4b8\n",
    "dict_content[\"1002674143_1b742ab4b8\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos verificar se as legendas foram mapeadas corretamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Images path\n",
    "image_path = \"/media/datasets/ComputerVision/Cap10/dados/imagens/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the image of ID equal to 15\n",
    "image_id = caption[15].split('.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Associating images and subtitles\n",
    "img = cv2.imread(image_path + image_id + \".jpg\")\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Caption:\")\n",
    "for caption in dict_content[image_id]:\n",
    "    print(caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtitle Data Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Data clearing \n",
    "def caption_cleaner(data):\n",
    "    \n",
    "    # Converte tudo para minúsculo\n",
    "    data = data.lower()\n",
    "\n",
    "    # Tudo que não for caracter será convertido para espaço\n",
    "    data = re.sub(\"[^a-z]+\", \" \", data)\n",
    "    \n",
    "    # Retorna somente sentenças com comprimento maior que 1\n",
    "    data = data.split()\n",
    "    data = [s for s in data if len(s) > 1]\n",
    "    data = \" \".join(data)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Testing\n",
    "print(caption_cleaner(\"The white 3 and brown # dog is running over the surface of the snow.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to all subtitles\n",
    "for key, value in dict_content.items():\n",
    "    for i in range(len(value)):\n",
    "        value[i] = caption_cleaner(value[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Result\n",
    "print(dict_content[\"1000268201_693b08cb0e\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the clean data on disk\n",
    "with open (\"dados/texto/tokens_clean.txt\", \"w\") as file:\n",
    "    file.write(str(dict_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Python set for the vocabulary (sets are unordered objects that accept any data type)\n",
    "vocab = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the subtitles to prepare vocabulary\n",
    "for key in dict_content.keys():\n",
    "    [vocab.update(sentence.split()) for sentence in dict_content[key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Vocabulary Size (Number of Individual Words): %d\"% len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View vocabulary\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of word occurrences considering all sentences.\n",
    "total_words = []\n",
    "for key in dict_content.keys():\n",
    "    [total_words.append(i) for des in dict_content[key] for i in des.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total Occurrences of Words:\" , len(total_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the total number of occurrences\n",
    "total_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying a filter to vocabulary based on word frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a counter from total words\n",
    "counter = collections.Counter(total_words)\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert count to dictionary\n",
    "freq_cnt = dict(counter)\n",
    "print(len(freq_cnt.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View Data\n",
    "freq_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the dictionary according to frequency\n",
    "sorted_freq_cnt = sorted(freq_cnt.items(), reverse = True, key = lambda x:x[1])\n",
    "sorted_freq_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the filter returning only words with a frequency greater than 10\n",
    "threshold = 10\n",
    "sorted_freq_cnt  = [x for x in sorted_freq_cnt if x[1] > threshold]\n",
    "total_words = [x[0] for x in sorted_freq_cnt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(total_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Training and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading image ids\n",
    "training_files = read_file(\"/media/datasets/ComputerVision/Cap10/dados/texto/trainImages.txt\")\n",
    "testing_files = read_file(\"/media/datasets/ComputerVision/Cap10/dados/texto/testImages.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate and list sentences\n",
    "training_data = [row.split(\".\")[0] for row in training_files.split(\"\\n\")[:-1]]\n",
    "testing_data = [row.split(\".\")[0] for row in testing_files.split(\"\\n\")[:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize data (image ids)\n",
    "training_files[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary for training content\n",
    "training_content = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through training data\n",
    "for img_id in training_files:\n",
    "    training_content[img_id] = []\n",
    "    for cap in dict_content[img_id]:\n",
    "        cap_to_append = \"startseq \" + cap + \" endseq\"\n",
    "        training_content[img_id].append(cap_to_append)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "training_content[\"2638369467_8fc251595b\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computer Vision - Xception Model for Feature Extraction\n",
    "\n",
    "[Xception](https://openaccess.thecvf.com/content_cvpr_2017/papers/Chollet_Xception_Deep_Learning_CVPR_2017_paper.pdf): Deep Learning with Depthwise Separable Convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer learning from Xception model with imagenet weights\n",
    "model = Xception(weights = 'imagenet', input_shape = (299, 299, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes the \"head\" (last two layers) of the original model\n",
    "new_model = Model(model.input, model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Summary of the new model\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing Images in the Xception Model Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for image processing\n",
    "def image_processing(img):\n",
    "    \n",
    "    # Upload an image\n",
    "    img = image.load_img(img, target_size = (299, 299))\n",
    "    img = image.img_to_array(img)\n",
    "\n",
    "    # Convert 3D tensor to 4D\n",
    "    img = np.expand_dims(img, axis = 0)\n",
    "\n",
    "    # Normalizes images according to Xception architecture requirements\n",
    "    img = preprocess_input(img)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the image processing function\n",
    "img = image_processing(\"/media/datasets/ComputerVision/Cap10/dados/imagens/2638369467_8fc251595b.jpg\")\n",
    "print(img.shape)\n",
    "plt.imshow(img[0])\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Images Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that receives an image and returns its encoding (resource vector)\n",
    "# Note that we are using \"new_model\" predictions to generate the feature vector\n",
    "def encode_image(img):\n",
    "    img = image_processing(img)\n",
    "    feature_vector = new_model.predict(img)\n",
    "    feature_vector = feature_vector.reshape((-1,))\n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the encoding function\n",
    "encode_image(image_path + \"1000268201_693b08cb0e.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary for image ids and training resource vectors\n",
    "encoding_treino = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Looping and encoding training data\n",
    "for img_id in training_data:\n",
    "    try:\n",
    "        PATH = image_path + img_id + \".jpg\"\n",
    "        encoding_treino[img_id] = encode_image(PATH)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the encoding result on disk \n",
    "with open(\"dados/encoders/atributos_treino_encoded.pkl\", \"wb\") as file:\n",
    "    pickle.dump(encoding_treino, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary for image ids and test resource vectors\n",
    "encoding_teste = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Loop and encoding in test data\n",
    "for img_id in testing_data:\n",
    "    try:\n",
    "        PATH = image_path + img_id + \".jpg\"\n",
    "        encoding_teste[img_id] = encode_image(PATH)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the encoding result on disk\n",
    "with open(\"dados/encoders/atributos_teste_encoded.pkl\", \"wb\") as file:\n",
    "    pickle.dump(encoding_teste, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Subtitles for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary\n",
    "len(total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word/Index/Word Mapping Dictionaries\n",
    "word_to_index = {}\n",
    "index_to_word = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop to fill dictionaries\n",
    "for i, palavra in enumerate(total_words):\n",
    "    word_to_index[palavra] = i + 1\n",
    "    index_to_word[i + 1] = palavra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(index_to_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also added the startseq and endseq tokens to the mappings to mark the beginning and end of sentences\n",
    "index_to_word[1846] = 'startseq'\n",
    "word_to_index['startseq'] = 1846\n",
    "index_to_word[1847] = 'endseq'\n",
    "word_to_index['endseq'] = 1847"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TVocabulary Size\n",
    "len(word_to_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the maximum length of a caption\n",
    "max_len = 0 \n",
    "\n",
    "for key in training_content.keys():\n",
    "    for cap in training_content[key]:\n",
    "        max_len = max(max_len, len(cap.split()))\n",
    "        \n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "Loading the Word Embeddings (numeric arrays that represent the words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contains 50-dimensional embeddings for 6 billion English words\n",
    "arquivo = open(\"/media/datasets/ComputerVision/Cap10/dados/glove/glove.6B.50d.txt\", encoding = 'utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to map words to Embeddings\n",
    "word_to_embedding = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary for Mapping Embeddings to Indexes\n",
    "embedding_index = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop to fill embedding_index\n",
    "for line in file:\n",
    "    \n",
    "    # Line split extracted from Glove\n",
    "    values = line.split()\n",
    "    \n",
    "    # Extract a value from Glove\n",
    "    word = values[0]\n",
    "    \n",
    "    # Extract word embedding from Glove\n",
    "    word_embedding = np.array(values[1:], dtype = 'float')\n",
    "    \n",
    "    # Feed the array with word embedding\n",
    "    embedding_index[word] = word_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closing files\n",
    "arquivo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "embedding_index['apple']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings Dimension\n",
    "EMBEDDING_DIM = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary Size\n",
    "VOCAB_SIZE = len(word_to_index) + 1\n",
    "print(\"Vocabulary Size:\", VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Embeddings Matrix\n",
    "def get_embedding_matrix():\n",
    "    \n",
    "    # Creates array of zeros as dimensions (VOCAB_SIZE, EMBEDDING_DIM)\n",
    "    embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "    \n",
    "    # Loop through the mapping to fill the Embeddings vector\n",
    "    for word, idx in word_to_index.items():\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        \n",
    "        # Load the Embeddings matrix\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[idx] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the function and get the Embeddings array\n",
    "embedding_matrix = get_embedding_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings matrix shape\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model Architecture\n",
    "\n",
    "They will be two parts of the same model:\n",
    "- Part 1 - Image Prediction\n",
    "- Part 2 - Subtitle Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input layer of images with shape 2048\n",
    "input_img_features = Input(shape = (2048,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout layer to smooth the model and avoid overfitting\n",
    "inp_img1 = Dropout(0.3)(input_img_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected dense layer for image predictions\n",
    "inp_img2 = Dense(256, activation = 'relu')(inp_img1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtitles input layer with max_len shape\n",
    "input_captions = Input(shape = (max_len,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding Layer\n",
    "inp_cap1 = Embedding(input_dim = VOCAB_SIZE, output_dim = EMBEDDING_DIM, mask_zero = True)(input_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout layer to smooth the model and avoid overfitting\n",
    "inp_cap2 = Dropout(0.3)(inp_cap1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Layer\n",
    "inp_cap3 = LSTM(256)(inp_cap2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs Decode \n",
    "- 1- An image (299x299x3) passes through the model.\n",
    "\n",
    "- 2- The final output is inp_img2 which now goes through the decoder (cell below).\n",
    "\n",
    "- 3- Similarly for subtitles that initially have a shape (batch_size x max_len).\n",
    "\n",
    "- 4- Next, after passing the subtitles through the Embeddings layer, we generate output as (batch_size x max_len x 50 (embedding_size))) and then it passes through the LSTM layer above and outputs as inp_cap3 (a 256-dimensional vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Associating images with captions for model learning\n",
    "decoder1 = Add()([inp_img2, inp_cap3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image/caption forecast result\n",
    "decoder2 = Dense(256, activation = 'relu')(decoder1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model output\n",
    "outputs = Dense(VOCAB_SIZE, activation = 'softmax')(decoder2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adds the input and output data format to the model as input\n",
    "final_model = Model(inputs = [input_img_features, input_captions], outputs = outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summmary of the model\n",
    "final_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-initializes the Embedding layer\n",
    "final_model.layers[2].set_weights([embedding_matrix])\n",
    "final_model.layers[2].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "final_model.compile(optimizer = \"adam\", loss = \"categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader (Data Generator to Train the Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generator\n",
    "def data_gen(conteudo_treino, encoding_treino, word_to_idx, max_len, batch_size):\n",
    "    \n",
    "    # image, subtitle, label\n",
    "    X1, X2, y = [], [], []\n",
    "    \n",
    "    # Counter\n",
    "    n = 0\n",
    "    \n",
    "    # Loop\n",
    "    while True:\n",
    "        \n",
    "        # Extract image id and caption\n",
    "        for key, desc_list in conteudo_treino.items():\n",
    "            \n",
    "            # Update the counter\n",
    "            n += 1\n",
    "            \n",
    "            # Extract the encoding from the image\n",
    "            try:\n",
    "                photo = encoding_treino[key]\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Extract captions for the image from this loopp pass\n",
    "            for desc in desc_list:\n",
    "                \n",
    "                # Sequence\n",
    "                seq = [word_to_idx[word] for word in desc.split() if word in word_to_idx]\n",
    "                \n",
    "                for i in range(1,len(seq)):\n",
    "                    \n",
    "                    # Input word string\n",
    "                    xi = seq[0:i]\n",
    "                    \n",
    "                    # Output word string\n",
    "                    yi = seq[i]\n",
    "                    \n",
    "                    # Add zero padding to the length of input strings\n",
    "                    # We take the first row only, since this method inserts and returns a 2D array\n",
    "                    xi = pad_sequences([xi], maxlen = max_len, value = 0, padding = 'post')[0]\n",
    "                    \n",
    "                    # Convert the expected word into One Hot vector notation\n",
    "                    yi = to_categorical([yi], num_classes = VOCAB_SIZE)[0]\n",
    "                    \n",
    "                    # Append\n",
    "                    X1.append(photo)\n",
    "                    X2.append(xi)\n",
    "                    y.append(yi)\n",
    "                    \n",
    "                # Update the variables\n",
    "                if n==batch_size:\n",
    "                    yield ([np.array(X1), np.array(X2)], np.array(y))\n",
    "                    X1,X2,y = [],[],[]\n",
    "                    n = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_epochs = 50\n",
    "batch_size = 3\n",
    "steps = len(training_content) // batch_size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data generator\n",
    "generator = data_gen(training_content, encoding_treino, word_to_index, max_len, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "final_model.fit(generator, epochs = num_epochs, steps_per_epoch = steps, verbose = 1)\n",
    "print(\"\\nTraining Completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "final_model.save('modelos/modelo_final.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict subtitles from images\n",
    "def caption_generator(img):\n",
    "    \n",
    "    # Mark start of text\n",
    "    in_text = 'startseq'\n",
    "    \n",
    "    # Loop\n",
    "    for i in range(max_len):\n",
    "        \n",
    "        # Text strings\n",
    "        sequence = [word_to_index[w] for w in in_text.split() if w in word_to_index]\n",
    "        sequence = pad_sequences([sequence], maxlen = max_len, padding = 'post')\n",
    "        \n",
    "        # Prediction\n",
    "        ypred = final_model.predict([img, sequence])\n",
    "        \n",
    "        # Get the highest probability prediction\n",
    "        ypred = ypred.argmax()\n",
    "        \n",
    "        # index for word\n",
    "        word = index_to_word[ypred]\n",
    "        in_text += (' ' +  word)\n",
    "        \n",
    "        # Check if it is end of sequence\n",
    "        if word == \"endseq\":\n",
    "            break\n",
    "    \n",
    "    # Final caption\n",
    "    legenda_final = in_text.split()[1:-1]\n",
    "    legenda_final = \" \".join(legenda_final)\n",
    "    \n",
    "    return legenda_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview and generation of captions for images\n",
    "\n",
    "# Style of images\n",
    "plt.style.use(\"seaborn\")\n",
    "\n",
    "# Loop through 10 images\n",
    "for i in range(11):\n",
    "    \n",
    "    # Generate an automatic id to fetch an image at random\n",
    "    idx = np.random.randint(0, 1000)\n",
    "    \n",
    "    # Encoding\n",
    "    all_img_names = list(encoding_teste.keys())\n",
    "    \n",
    "    # Image ID \n",
    "    img_name = all_img_names[idx]\n",
    "    \n",
    "    # Image to preview the subtitle\n",
    "    photo_2048 = encoding_teste[img_name].reshape((1,2048))\n",
    "    \n",
    "    # CLoad image from disk\n",
    "    i = plt.imread(\"/media/datasets/ComputerVision/Cap10/dados/imagens/\" + img_name + \".jpg\")\n",
    "    \n",
    "    # Subtitle preview\n",
    "    legenda = caption_generator(photo_2048)\n",
    "    \n",
    "    # Print\n",
    "    plt.title(legenda)\n",
    "    plt.imshow(i)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
